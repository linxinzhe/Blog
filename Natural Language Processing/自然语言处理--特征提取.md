自然语言中意义最小的单位就是单词，其次是句子，再是段落，最后一整篇文章。
通常来说，提取单词的特征是最常用的提取方法。当然，特征提取也是根据你看问题的角度的来决定，也就是说你要解决的问题以及解决问题的模型所决定的。

下面我们仅仅从单词角度来看问题来说说两种常见的特征表示的方法。

词袋：
最简单的是一种叫做词袋的特征。对于一篇文章，每个单词都收入一个词袋中并计数，比如在一篇介绍狗的百度百科文章中，“狗”出现10次，“犬”出现8次。则词包是{“狗”：10，“犬”：8}。
词袋的用途可以在文章分类时，相同类型的文章，词出现的频率也是差不多的。

但是存在一个问题，文章有长有短，文章长明显词就长得多，为了能比较不同长度的文章，因此词袋中的词出现的频率要做正则化。用的方法叫TF-IDF，也就是词出现的频率要乘以词的权重，这样就能统一比较不同长度的文章了。

词向量：
我们再来看另一种单词的特征表达方法，叫做词向量。单词自身是存在意义的，而且在文章的上下文语意下的意思也不同。因此单纯的词袋方法，不能解决较复杂的场景。这时候就要用到词向量，一个词对应一个多维的向量，在得到词向量后可以认为计算机理解了这个词的多种语意。最神奇的是，词向量还存在可以加减性，例如：“小猫”-“猫”+“狗”=“小狗”。可以认为，向量的某个维度反应了大小特征，另一个反映了动物特征，则减去了猫的特征再加狗的特征只变动了动物特征，就把这个词向量变为了“小狗"

这里读者肯定好奇这么好的词向量方法怎么从文章中提取出来呢？

这里说一种用深度学习得到词向量的方法，叫word2vec。
首先，我们需要准备一大堆文章，俗称语料。
然后，按句子的顺序一次次输入单词，每次输入一个词，让深度学习的模型预测这个词在句子中的周围的词，并反馈预测的对错。
最后，在大量的语料的输入和反馈下，模型就逐渐知道一个词在上下文中的含义了。

最后的最后，来看一个好玩的东西，一个词向量通常是多维度的，我们用一种叫t-SNE的方法把他压缩成二位平面上的图，你能看到，相近意思的词会聚拢在一起。

